# -*- coding: utf-8 -*-
"""Taller_practico_EDA_gym.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EE77yPaQRPR34lt91pd77-Iyw7ASljCM

#2 taller
### gym_members_exercise.csv

Parte 1: Preguntas Teóricas
Analizar en detalle el dataset y dar respuesta a las siguientes preguntas:
1.1 Describirlo con sus propias palabras.
- R: El dataset contiene datos de miembros de un gimnasio como edad, género, peso, altura, frecuencia cardíaca, duración de ejercicio, calorías quemadas, tipo de entrenamiento y nivel de experiencia.
1.2 Definir 5 potenciales casos de uso que se puedan resolver con ML haciendo uso del dataset.
* Predecir calorías quemadas según variables físicas.

* Clasificar nivel de experiencia del usuario.

* Recomendar el tipo de entrenamiento.

* Agrupar usuarios similares con clustering.

* Detectar usuarios con rutinas anómalas.





1.2 Casos de uso con Machine Learning:

Predecir calorías quemadas según variables físicas.

Clasificar nivel de experiencia del usuario.

Recomendar el tipo de entrenamiento.

Agrupar usuarios similares con clustering.

Detectar usuarios con rutinas anómalas.


---
1. Exploración inicial
- ¿Cuántas filas y columnas tiene el dataset? Muestra las primeras 5 filas.
2. Limpieza básica
- ¿Hay valores nulos en el dataset? Si es así, ¿cuántos por columna?
3. Imputación de Datos
- Utilice una estrategia de las aprendidas hasta el momento en el bootcamp
para imputar cada uno de los datos que ha encontrado nulos, hágalo por
cada feature, expliqué en cada cada caso por qué seleccionó cada
estrategia.
4. Análisis Univariado
- Realicé un análisis univariado de los features numéricos que presenta el
dataset, por favor sacar cinco conclusiones de lo observado.
5. Codificación de features
- Realicé un análisis y defina cuáles de los features categóricos que contiene
el dataset deben ser codificados antes de empezar a realizar el análisis
Análisis Bivariado
6. Realicé un análisis bivariado de los features numéricos que presenta el
dataset, por favor sacar cinco conclusiones de lo observado
específicamente en la matriz de correlación.
7. Escalamiento de features
- Realicé un escalamiento de features del dataset, observé el resultado y
deduzca tres conclusiones de lo observado.
8. Correlación de features después de escalamiento
-  Una vez ha realizado el escalamiento por favor volver a crear la matriz de
correlación y observar cómo cambió con respecto el ejercicio anterior, sacar
tres conclusiones principales.
9. Calcule los datos nulos del feature Calories_Burned haciendo uso de regression
o Por favor vuelva a tomar el dataset original y enfóquese en el feature
Calories_Burned, a éste feature impute los datos nulos haciendo uso del
algoritmo de regresión visto durante clase. Cuando lo haga observe qué tal el
resultado y sugiera siguientes pasos de acuerdo a lo observado.
"""

import pandas as pd

df_gyms = pd.read_csv('gym_members_exercise.csv')

# ¿Cuántas filas y columnas tiene el dataset?
print("Dimensiones del dataset (filas, columnas):", df_gyms.shape)

# Mostrar las primeras 5 filas
df_gyms.head()

# Descripción estadística de las columnas numéricas
df_gyms.describe()

#2. Valores nulos en el dataframe

df_gyms.isnull().sum()

#3.Imputacion de datos
# Copia del DataFrame original
df_gym_imputacion = df_gyms.copy()

# Imputación por columna con estrategia y explicación

# Fat_Percentage: usamos la mediana porque puede haber outliers
df_gym_imputacion["Fat_Percentage"].fillna(df_gyms["Fat_Percentage"].median(), inplace=True)

# Calories_Burned: usamos la media porque los valores tienen una distribución relativamente normal
df_gym_imputacion["Calories_Burned"].fillna(df_gyms["Calories_Burned"].mean(), inplace=True)

# Workout_Type: categórico, imputamos con la moda (valor más frecuente)
df_gym_imputacion["Workout_Type"].fillna(df_gyms["Workout_Type"].mode()[0], inplace=True)

# Comparación antes y después
pd.DataFrame({
    "ANTES": df_gyms.isnull().sum(),
    "DESPUÉS": df_gym_imputacion.isnull().sum()
})

#4.Analisis Univariado

import matplotlib.pyplot as plt
import seaborn as sns

# Histograma para cada variable numérica
df_gyms.hist(figsize=(12, 8), bins=20)
plt.tight_layout()
plt.show()

"""### CONCLUSIONES ANALISIS UNIVARIADO

1.
"""

#5. Codificación de features
# Copia del DataFrame original para codificación
df_encoded = df_gyms.copy()

# 1. Codificación de 'Gender' (LabelEncoder / map)
df_encoded["Gender"] = df_encoded["Gender"].map({"Male": 0, "Female": 1})

# 2. Codificación de 'Workout_Type' (OneHotEncoding)
df_encoded = pd.get_dummies(df_encoded, columns=["Workout_Type"])

# 3. Conversión de 'Max_BPM' a tipo numérico (por si está mal importada)
df_encoded["Max_BPM"] = pd.to_numeric(df_encoded["Max_BPM"], errors="coerce")

# Verificamos que no haya errores
df_encoded.info()

# Punto 5 - Codificación de features (Método alternativo con get_dummies)

# Paso 1: Convertir las columnas categóricas a tipo "category"
# Esto le indica explícitamente a pandas que estas columnas no son numéricas, sino categóricas
categorical_columns = ["Gender", "Workout_Type"]
for col in categorical_columns:
    df_gyms[col] = df_gyms[col].astype("category")

# Paso 2: Aplicar One-hot Encoding sin eliminar ninguna categoría (drop_first=False)
# Esto conserva todas las clases, útil para evitar pérdida de información en análisis exploratorios
data_encoded = pd.get_dummies(df_gyms, columns=categorical_columns, drop_first=False)

# Paso 3: Asegurar que las columnas nuevas sean de tipo entero (0 y 1)
encoded_columns = data_encoded.columns.difference(df_gyms.columns)
data_encoded[encoded_columns] = data_encoded[encoded_columns].astype(int)

# Visualizamos el nuevo DataFrame codificado
data_encoded.head()

#6. Análisis bivariado

# Paso 1: Codificar las variables categóricas
categorical_columns = ["Gender", "Workout_Type"]
for col in categorical_columns:
    df_gyms[col] = df_gyms[col].astype("category")

# Paso 2: One-hot Encoding
data_encoded = pd.get_dummies(df_gyms, columns=categorical_columns, drop_first=False)

# Paso 3: Convertir las columnas codificadas a enteros (0 y 1)
encoded_columns = data_encoded.columns.difference(df_gyms.columns)
data_encoded[encoded_columns] = data_encoded[encoded_columns].astype(int)


# Paso 4: Seleccionar solo las columnas numéricas
numerical_data = data_encoded.select_dtypes(include='number')

# Paso 5: Calcular la matriz de correlación
correlation_matrix = numerical_data.corr()

# Paso 6: Visualizar el mapa de calor
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Matriz de correlación entre variables numéricas")
plt.tight_layout()
plt.show()

# Mostrar la matriz para inspección adicional
correlation_matrix.round(2)

#7. Escalamiento de features

from sklearn.preprocessing import MinMaxScaler

# Selección de columnas numéricas a escalar
numerical_features = [
    "Session_Duration (hours)",
    "Calories_Burned",
    "Fat_Percentage",
    "Water_Intake (liters)",
    "Workout_Frequency (days/week)",
    "BMI",
    "Experience_Level"
]

# Aplicar MinMaxScaler (valores entre 0 y 1)
scaler = MinMaxScaler()
data_scaled = data_encoded.copy()
data_scaled[numerical_features] = scaler.fit_transform(data_scaled[numerical_features])

# Descripción de las columnas escaladas
data_scaled[numerical_features].describe().round(2)